# Patch Plan: `run_20260223_013940`

## 1. Fix metadata extraction bug in in-container grader
- File: `adapters/swebench/swebench_task_container_grader.py`
- Update `extract_swebench_meta()` to support both payload schemas:
  - `task.swebench.input.*` (trial-input shape)
  - `swebench.input.*` (task-boundary shape)
- Add fallback reads for:
  - `task.input.instance_id`
  - `input.instance_id`
- Keep existing behavior unchanged when fields are already present.

## 2. Add regression tests for both payload schemas
- Add tests for `extract_swebench_meta()` that cover:
  - nested `task.swebench.input`
  - top-level `swebench.input`
  - missing/invalid fields (must return `None` values)
- Assert `benchmark_prediction_record_v1.ext.swebench.instance_id` is non-null for valid fixtures.

## 3. Align extraction logic in official adapter
- File: `adapters/swebench/swebench_official_benchmark_adapter.py`
- Ensure extraction behavior is consistent with the in-container grader.
- Avoid schema-specific drift by sharing equivalent fallback order.

## 4. Harden run wrapper finalization checks
- File: `scripts/agentlab/run_curated_experiment.sh`
- After `lab-cli` exits, require terminal run state before returning success.
- If `runtime/run_control.json` remains `"status": "running"`, return non-zero and print diagnostics.
- Preserve run logs on completion/failure for debugging instead of cleaning immediately.

## 5. Add reconciliation utility for partial ingestion
- New file: `scripts/agentlab/reconcile_benchmark_records.sh`
- Rebuild run-level files from trial outputs:
  - `benchmark/predictions.jsonl` from `trials/trial_*/out/benchmark_prediction.json`
  - `benchmark/scores.jsonl` from `trials/trial_*/out/benchmark_score.json`
- Deterministically sort by `trial_id` (or schedule index if available).
- Include guardrails for missing trial files and duplicate trial IDs.

## 6. Validate with smoke run first
- Run `AGENTLAB_LIMIT=1` before full rerun.
- Acceptance criteria:
  - `trial_*/out/benchmark_prediction.json` has non-null `ext.swebench.instance_id`
  - run-level `benchmark/scores.jsonl` line count equals completed trial count
  - run state reaches terminal status (`completed` or `failed`), not `running`

## Suggested Execution Order
1. Implement step 1.
2. Add tests from step 2.
3. Align official adapter (step 3).
4. Add wrapper hardening (step 4).
5. Add reconciliation script (step 5).
6. Run smoke validation (step 6), then full run.
